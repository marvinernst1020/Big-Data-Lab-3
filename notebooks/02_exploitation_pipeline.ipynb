{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7f33fd",
   "metadata": {},
   "source": [
    "# Lab 3 - Data Exploitation Pipeline\n",
    "\n",
    "**Notebook:** `02_exploitation_pipeline.ipynb`  \n",
    "**Objective:** We implement the **second pipeline**: transform cleaned data in the Formatted Zone into aggregated and enriched outputs in the Exploitation Zone, and validate the results. This should enable subsequent analysis of KPIs across space and time.\n",
    "\n",
    "### Group: *L3-T04*\n",
    "\n",
    "#### Group Members: **Marvin Ernst, Oriol Gelabert, Alex Malo**  \n",
    "Class: **23D020 - Big Data Management for Data Science**  \n",
    "Date: *June 23, 2025*\n",
    "\n",
    "\n",
    "In this notebook we include the following two parts of the project (from section A):\n",
    "- A.3: Move Data to the Exploitation Zone\n",
    "- A.4: Validate the Data\n",
    "\n",
    "---\n",
    "### Steps in this pipeline:\n",
    "1. **Create Exploitation Zone**  \n",
    "   Define the directory and prepare structures for output.\n",
    "\n",
    "2. **Data Aggregation and Transformation**  \n",
    "   - Aggregate household size, commercial premises, and tourist housing datasets  \n",
    "   - Calculate shares and total counts per spatial unit and year\n",
    "\n",
    "3. **Data Validation**  \n",
    "   Check schema, nulls, uniqueness, and summary statistics of outputs to ensure readiness for analysis.\n",
    "---\n",
    "\n",
    "First, we load the necessary libraries and set up the folder structure. All file paths are relative, ensuring portable execution of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc08cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, countDistinct, isnan, when, sum as _sum, round\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20772a0",
   "metadata": {},
   "source": [
    "#### Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54937f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/24 02:51:30 WARN Utils: Your hostname, MacBook-Pro-40.local resolves to a loopback address: 127.0.0.1; using 192.168.1.23 instead (on interface en0)\n",
      "25/06/24 02:51:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/24 02:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/24 02:51:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Exploitation Pipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a03cb",
   "metadata": {},
   "source": [
    "#### Define Folder Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb8df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path().resolve().parent\n",
    "formatted = project_root / \"formatted_zone\"\n",
    "exploitation = project_root / \"exploitation_zone\"\n",
    "exploitation.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a451c77",
   "metadata": {},
   "source": [
    "## **A.3 - Move Data to the Exploitation Zone**\n",
    "\n",
    "This section generates pre-aggregated, spatially-aware indicators from each dataset:\n",
    "\n",
    "- **Tourist Housing**: counts by district and neighborhood per quarter  \n",
    "- **Household Size** : shares of households by size and level (AEB, neighborhood, district)  \n",
    "- **Commercial Premises**: percentages of selected economic indicators per zone\n",
    " \n",
    "These tables will be saved as `.parquet` files in the exploitation zone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b767c7c6",
   "metadata": {},
   "source": [
    "#### **Household Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0fa135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# First, we define a pattern to identify household parquet files by year:\n",
    "pattern = re.compile(r\"household_(20\\d{2})_\\.parquet\", re.IGNORECASE)\n",
    "\n",
    "# We initialize dictionaries to store results by aggregation level:\n",
    "df_households = {}\n",
    "df_households_aeb = {}\n",
    "df_households_barri = {}\n",
    "df_households_districte = {}\n",
    "df_totals_dict = {}\n",
    "\n",
    "# We loop through each file in the formatted zone:\n",
    "for file in os.listdir(formatted):\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        filepath = os.path.join(formatted, file)\n",
    "\n",
    "        try:\n",
    "            # Load each household file from the formatted zone:\n",
    "            df = spark.read.parquet(filepath)\n",
    "\n",
    "            # We compute a new column multiplying persons per household size:\n",
    "            df = df.withColumn(\"TOTAL_PERSONES_AGG\", col(\"NUM_PERSONES_AGG\") * col(\"NUM_VALOR\"))\n",
    "\n",
    "            # We define grouping levels of interest:\n",
    "            group_cols = [\"COD_BARRI\", \"COD_AEB\", \"COD_DISTRICTE\", \"COD_SECCIO_CENSAL\"]\n",
    "\n",
    "            # For each spatial level, compute total persons and dwellings:\n",
    "            for col_name in group_cols:\n",
    "                sum_per_col = f\"SUM_TOTAL_PERSONES_{'_'.join(col_name.split('_')[1:])}\"\n",
    "                pct_per_col = f\"PCT_PERSONES_{'_'.join(col_name.split('_')[1:])}\"\n",
    "                sum_viv_col = f\"SUM_VIVENDES_{'_'.join(col_name.split('_')[1:])}\"\n",
    "                pct_viv_col = f\"PCT_VIVENDES_{'_'.join(col_name.split('_')[1:])}\"\n",
    "\n",
    "                # Aggregate total persons and dwellings per group:\n",
    "                df_totals = df.groupBy(col_name).agg(\n",
    "                    _sum(\"TOTAL_PERSONES_AGG\").alias(sum_per_col),\n",
    "                    _sum(\"NUM_VALOR\").alias(sum_viv_col)\n",
    "                )\n",
    "\n",
    "                # Save aggregated totals in case we want to reuse them:\n",
    "                df_totals_dict[f\"{'_'.join(col_name.split('_')[1:])}_{year}\"] = df_totals\n",
    "\n",
    "                # Join back with original dataframe to calculate percentages:\n",
    "                df = df.join(df_totals, on=col_name, how=\"left\")\n",
    "                df = df.withColumn(pct_per_col, round(100 * col(\"TOTAL_PERSONES_AGG\") / col(sum_per_col), 3))\n",
    "                df = df.withColumn(pct_viv_col, round(100 * col(\"NUM_VALOR\") / col(sum_viv_col), 3))\n",
    "\n",
    "            # Aggregate household data by AEB level:\n",
    "            df_households_aeb[year] = df.groupBy(\"COD_AEB\", \"NUM_PERSONES_AGG\").agg(\n",
    "                _sum(\"TOTAL_PERSONES_AGG\").alias(\"TOTAL_PERSONES_AGG\"),\n",
    "                _sum(\"NUM_VALOR\").alias(\"NUM_VALOR\"),\n",
    "                round(_sum(\"PCT_PERSONES_AEB\"), 3).alias(\"PCT_PERSONES_AEB\"),\n",
    "                round(_sum(\"PCT_VIVENDES_AEB\"), 3).alias(\"PCT_VIVENDES_AEB\")\n",
    "            )\n",
    "\n",
    "            # Aggregate household data by neighborhood level:\n",
    "            df_households_barri[year] = df.groupBy(\"COD_BARRI\", \"DES_BARRI\", \"NUM_PERSONES_AGG\").agg(\n",
    "                _sum(\"TOTAL_PERSONES_AGG\").alias(\"TOTAL_PERSONES_AGG\"),\n",
    "                _sum(\"NUM_VALOR\").alias(\"NUM_VALOR\"),\n",
    "                round(_sum(\"PCT_PERSONES_BARRI\"), 3).alias(\"PCT_PERSONES_BARRI\"),\n",
    "                round(_sum(\"PCT_VIVENDES_BARRI\"), 3).alias(\"PCT_VIVENDES_BARRI\")\n",
    "            )\n",
    "\n",
    "            # Aggregate household data by district level:\n",
    "            df_households_districte[year] = df.groupBy(\"COD_DISTRICTE\", \"DES_DISTRICTE\", \"NUM_PERSONES_AGG\").agg(\n",
    "                _sum(\"TOTAL_PERSONES_AGG\").alias(\"TOTAL_PERSONES_AGG\"),\n",
    "                _sum(\"NUM_VALOR\").alias(\"NUM_VALOR\"),\n",
    "                round(_sum(\"PCT_PERSONES_DISTRICTE\"), 3).alias(\"PCT_PERSONES_DISTRICTE\"),\n",
    "                round(_sum(\"PCT_VIVENDES_DISTRICTE\"), 3).alias(\"PCT_VIVENDES_DISTRICTE\")\n",
    "            )\n",
    "\n",
    "            # Drop intermediate columns used for joins to clean up the final DataFrame:\n",
    "            df_households[year] = df.drop(\n",
    "                'SUM_TOTAL_PERSONES_BARRI', 'SUM_VIVENDES_BARRI', 'PCT_PERSONES_BARRI', 'PCT_VIVENDES_BARRI',\n",
    "                'SUM_TOTAL_PERSONES_AEB', 'SUM_VIVENDES_AEB', 'PCT_PERSONES_AEB', 'PCT_VIVENDES_AEB',\n",
    "                'SUM_TOTAL_PERSONES_DISTRICTE', 'SUM_VIVENDES_DISTRICTE', 'PCT_PERSONES_DISTRICTE',\n",
    "                'PCT_VIVENDES_DISTRICTE', 'SUM_TOTAL_PERSONES_SECCIO_CENSAL', 'SUM_VIVENDES_SECCIO_CENSAL'\n",
    "            )\n",
    "\n",
    "            # Finally, we save all three aggregation levels into the exploitation zone:\n",
    "            df_households_aeb[year].write.parquet(f\"{exploitation}/households_aeb_{year}.parquet\", mode='overwrite')\n",
    "            df_households_barri[year].write.parquet(f\"{exploitation}/households_barri_{year}.parquet\", mode='overwrite')\n",
    "            df_households_districte[year].write.parquet(f\"{exploitation}/households_districte_{year}.parquet\", mode='overwrite')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing household file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b493376",
   "metadata": {},
   "source": [
    "#### **Commercial Premises**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b76394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved commercial indicators for 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved commercial indicators for 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 141:============================>                           (9 + 8) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved commercial indicators for 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Again, we define a pattern to match formatted commercial files by year:\n",
    "pattern = re.compile(r\"comercial_(20\\d{2})\\.parquet\", re.IGNORECASE)\n",
    "\n",
    "# We initialize two dictionaries: one for raw commercial data and one for aggregated indicators:\n",
    "df_comercial = {}\n",
    "df_indicators = {}\n",
    "\n",
    "# We loop through each commercial parquet file found in the formatted zone:\n",
    "for file in os.listdir(formatted):\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        filepath = os.path.join(formatted, file)\n",
    "\n",
    "        try:\n",
    "            # Load the cleaned commercial data from the formatted zone:\n",
    "            df = spark.read.parquet(filepath)\n",
    "\n",
    "            # We define the list of indicator columns to summarize:\n",
    "            indicator_cols = [\n",
    "                \"IND_OCI_NOCTURN\", \"IND_COWORKING\", \"IND_SERVEI_DEGUSTACIO\", \"IND_OBERT24H\",\n",
    "                \"IND_MIXT\", \"IND_PEU_CARRER\", \"IND_MERCAT\", \"IND_GALERIA\",\n",
    "                \"IND_CENTRE_COMERCIAL\", \"IND_EIX_COMERCIAL\"\n",
    "            ]\n",
    "\n",
    "            # First, we aggregate the indicators at the district level:\n",
    "            agg_df_dis = df.groupBy(\"DES_ACTIVITAT_PRINCIPAL\", \"DES_GRUP\", \"COD_DISTRICTE\").agg(\n",
    "                count(\"*\").alias(\"TOTAL\"),\n",
    "                *[_sum(c).alias(f\"TOTAL_{c}\") for c in indicator_cols]\n",
    "            )\n",
    "\n",
    "            # Next, we do a similar aggregation at the neighborhood level:\n",
    "            agg_df_barri = df.groupBy(\"DES_ACTIVITAT_PRINCIPAL\", \"DES_GRUP\", \"COD_DISTRICTE\", \"COD_BARRI\").agg(\n",
    "                count(\"*\").alias(\"TOTAL\"),\n",
    "                *[_sum(c).alias(f\"TOTAL_{c}\") for c in indicator_cols]\n",
    "            )\n",
    "\n",
    "            # Finally, we repeat the aggregation at the census section level:\n",
    "            agg_df_sec = df.groupBy(\"DES_ACTIVITAT_PRINCIPAL\", \"DES_GRUP\", \"COD_DISTRICTE\", \"COD_BARRI\", \"COD_SECCIO_CENSAL\").agg(\n",
    "                count(\"*\").alias(\"TOTAL\"),\n",
    "                *[_sum(c).alias(f\"TOTAL_{c}\") for c in indicator_cols]\n",
    "            )\n",
    "\n",
    "            # For each aggregation level, we compute the percentage of each indicator relative to the total:\n",
    "            for df_agg in [agg_df_dis, agg_df_barri, agg_df_sec]:\n",
    "                for c in indicator_cols:\n",
    "                    df_agg = df_agg.withColumn(f\"PCT_{c}\", round(100 * col(f\"TOTAL_{c}\") / col(\"TOTAL\"), 3))\n",
    "\n",
    "            # Here, unlike previous datasets, we write **three levels of aggregation** directly to the exploitation zone:\n",
    "            agg_df_dis.write.parquet(f\"{exploitation}/comercial_indicators_district_{year}.parquet\", mode=\"overwrite\")\n",
    "            agg_df_barri.write.parquet(f\"{exploitation}/comercial_indicators_barri_{year}.parquet\", mode=\"overwrite\")\n",
    "            agg_df_sec.write.parquet(f\"{exploitation}/comercial_indicators_section_{year}.parquet\", mode=\"overwrite\")\n",
    "\n",
    "            # We also store the processed DataFrames in dictionaries for potential reuse:\n",
    "            df_indicators[year] = agg_df_dis\n",
    "            df_comercial[year] = df\n",
    "\n",
    "            print(f\"Saved commercial indicators for {year}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing commercial file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3610e53",
   "metadata": {},
   "source": [
    "### **Tourist Housing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c154d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a pattern to match tourist housing files in the formatted zone, e.g., \"hut_2022_1T.parquet\":\n",
    "pattern = re.compile(r\"hut_(20\\d{2})_(\\dT)\\.parquet\", re.IGNORECASE)\n",
    "\n",
    "# We initialize dictionaries to store the full, district-level, and neighborhood-level data:\n",
    "df_hut = {}\n",
    "df_hut_dis = {}\n",
    "df_hut_barri = {}\n",
    "\n",
    "# We iterate through each file in the formatted directory that matches our naming pattern:\n",
    "for file in os.listdir(formatted):\n",
    "    match = pattern.match(file)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        term = match.group(2)\n",
    "        filepath = os.path.join(formatted, file)\n",
    "\n",
    "        try:\n",
    "            # We load the cleaned tourist housing file:\n",
    "            df = spark.read.parquet(filepath)\n",
    "\n",
    "            # We compute the number of licenses per district:\n",
    "            agg_df_dis = df.groupBy('COD_DISTRICTE').agg(count(\"*\").alias(\"TOTAL\"))\n",
    "\n",
    "            # We also compute the number of licenses per district and neighborhood:\n",
    "            agg_df_barri = df.groupBy('COD_DISTRICTE', 'DES_BARRI').agg(count(\"*\").alias(\"TOTAL\"))\n",
    "\n",
    "            # We store the full and aggregated DataFrames in our dictionaries for further use:\n",
    "            df_hut[f\"{year}_{term}\"] = df\n",
    "            df_hut_dis[f\"{year}_{term}\"] = agg_df_dis\n",
    "            df_hut_barri[f\"{year}_{term}\"] = agg_df_barri\n",
    "\n",
    "            # We save the aggregated outputs into the exploitation zone for later analysis:\n",
    "            agg_df_dis.write.parquet(f\"{exploitation}/hut_district_{year}_{term}.parquet\", mode='overwrite')\n",
    "            agg_df_barri.write.parquet(f\"{exploitation}/hut_barri_{year}_{term}.parquet\", mode='overwrite')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tourist housing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d12c8",
   "metadata": {},
   "source": [
    "## **A.4 - Validate the Data**\n",
    "\n",
    "In this section, we **validate the integrity and usability** of the datasets in the **Exploitation Zone**:\n",
    "\n",
    "- Inspect schemas and row counts\n",
    "- Check for nulls and missing values\n",
    "- Validate key columns (e.g. IDs, zones, aggregates)\n",
    "- Ensure data is ready for KPI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83489e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploitation path: /Users/Admin/Documents/Git/BSE/BigData/Big-Data-Lab-3/exploitation_zone\n"
     ]
    }
   ],
   "source": [
    "exploitation = project_root / \"exploitation_zone\"\n",
    "print(f\"Exploitation path: {exploitation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bcfb9",
   "metadata": {},
   "source": [
    "#### **Household Size** (Barri Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90864d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COD_BARRI: integer (nullable = true)\n",
      " |-- DES_BARRI: string (nullable = true)\n",
      " |-- NUM_PERSONES_AGG: integer (nullable = true)\n",
      " |-- TOTAL_PERSONES_AGG: long (nullable = true)\n",
      " |-- NUM_VALOR: long (nullable = true)\n",
      " |-- PCT_PERSONES_BARRI: double (nullable = true)\n",
      " |-- PCT_VIVENDES_BARRI: double (nullable = true)\n",
      "\n",
      "+---------+--------------------+----------------+------------------+---------+------------------+------------------+\n",
      "|COD_BARRI|           DES_BARRI|NUM_PERSONES_AGG|TOTAL_PERSONES_AGG|NUM_VALOR|PCT_PERSONES_BARRI|PCT_VIVENDES_BARRI|\n",
      "+---------+--------------------+----------------+------------------+---------+------------------+------------------+\n",
      "|        4|Sant Pere, Santa ...|               6|               906|      151|             4.101|             1.473|\n",
      "|       33|    el Baix Guinardó|               4|              5716|     1429|            22.574|            13.295|\n",
      "|       13|   la Marina de Port|               6|              1458|      243|             4.804|             2.029|\n",
      "|       34|            Can Baró|               3|              2148|      716|            23.189|            18.223|\n",
      "|       54|          Torre Baró|               2|               484|      242|            16.759|            24.568|\n",
      "+---------+--------------------+----------------+------------------+---------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows: 657\n",
      "+---------+---------+----------------+------------------+---------+------------------+------------------+\n",
      "|COD_BARRI|DES_BARRI|NUM_PERSONES_AGG|TOTAL_PERSONES_AGG|NUM_VALOR|PCT_PERSONES_BARRI|PCT_VIVENDES_BARRI|\n",
      "+---------+---------+----------------+------------------+---------+------------------+------------------+\n",
      "|        0|        0|               0|                 0|        0|                 0|                 0|\n",
      "+---------+---------+----------------+------------------+---------+------------------+------------------+\n",
      "\n",
      "Unique group combinations (COD_BARRI, household size): 657\n"
     ]
    }
   ],
   "source": [
    "df_households_barri = spark.read.parquet(f\"{exploitation}/households_barri_2022.parquet\")\n",
    "\n",
    "# Print schema\n",
    "df_households_barri.printSchema()\n",
    "\n",
    "# Show sample rows\n",
    "df_households_barri.show(5)\n",
    "\n",
    "# Count total records\n",
    "print(\"Total rows:\", df_households_barri.count())\n",
    "\n",
    "# Check for null values\n",
    "df_households_barri.select([count(when(col(c).isNull(), c)).alias(c) for c in df_households_barri.columns]).show()\n",
    "\n",
    "# Check uniqueness of (COD_BARRI, NUM_PERSONES_AGG)\n",
    "print(\"Unique group combinations (COD_BARRI, household size):\",\n",
    "      df_households_barri.select(\"COD_BARRI\", \"NUM_PERSONES_AGG\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1371b6",
   "metadata": {},
   "source": [
    "#### **Commercial Premises Indicators** (District Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb650ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DES_ACTIVITAT_PRINCIPAL: string (nullable = true)\n",
      " |-- DES_GRUP: string (nullable = true)\n",
      " |-- COD_DISTRICTE: integer (nullable = true)\n",
      " |-- TOTAL: long (nullable = true)\n",
      " |-- TOTAL_IND_OCI_NOCTURN: long (nullable = true)\n",
      " |-- TOTAL_IND_COWORKING: long (nullable = true)\n",
      " |-- TOTAL_IND_SERVEI_DEGUSTACIO: long (nullable = true)\n",
      " |-- TOTAL_IND_OBERT24H: long (nullable = true)\n",
      " |-- TOTAL_IND_MIXT: long (nullable = true)\n",
      " |-- TOTAL_IND_PEU_CARRER: long (nullable = true)\n",
      " |-- TOTAL_IND_MERCAT: long (nullable = true)\n",
      " |-- TOTAL_IND_GALERIA: long (nullable = true)\n",
      " |-- TOTAL_IND_CENTRE_COMERCIAL: long (nullable = true)\n",
      " |-- TOTAL_IND_EIX_COMERCIAL: long (nullable = true)\n",
      "\n",
      "+-----------------------+--------------------+-------------+-----+---------------------+-------------------+---------------------------+------------------+--------------+--------------------+----------------+-----------------+--------------------------+-----------------------+\n",
      "|DES_ACTIVITAT_PRINCIPAL|            DES_GRUP|COD_DISTRICTE|TOTAL|TOTAL_IND_OCI_NOCTURN|TOTAL_IND_COWORKING|TOTAL_IND_SERVEI_DEGUSTACIO|TOTAL_IND_OBERT24H|TOTAL_IND_MIXT|TOTAL_IND_PEU_CARRER|TOTAL_IND_MERCAT|TOTAL_IND_GALERIA|TOTAL_IND_CENTRE_COMERCIAL|TOTAL_IND_EIX_COMERCIAL|\n",
      "+-----------------------+--------------------+-------------+-----+---------------------+-------------------+---------------------------+------------------+--------------+--------------------+----------------+-----------------+--------------------------+-----------------------+\n",
      "|                  Actiu| Parament de la llar|            3|  178|                    0|                  0|                          0|                 0|             0|                 168|              10|                0|                         0|                     59|\n",
      "|                  Actiu|Sanitat i assistè...|            1|   42|                    0|                  0|                          0|                 0|             0|                  38|               0|                4|                         0|                      7|\n",
      "|                  Actiu|Equipaments cultu...|            1|  182|                    0|                  5|                          0|                 0|             0|                 181|               0|                1|                         0|                     77|\n",
      "|                  Actiu|       Oci i cultura|            4|   70|                    0|                  0|                          0|                 0|             0|                  61|               0|                0|                         9|                     13|\n",
      "|                  Actiu| Parament de la llar|            9|  179|                    0|                  0|                          0|                 0|             0|                 170|               0|                0|                         9|                     38|\n",
      "+-----------------------+--------------------+-------------+-----+---------------------+-------------------+---------------------------+------------------+--------------+--------------------+----------------+-----------------+--------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows: 159\n",
      "+-----------------------+--------+-------------+-----+---------------------+-------------------+---------------------------+------------------+--------------+--------------------+----------------+-----------------+--------------------------+-----------------------+\n",
      "|DES_ACTIVITAT_PRINCIPAL|DES_GRUP|COD_DISTRICTE|TOTAL|TOTAL_IND_OCI_NOCTURN|TOTAL_IND_COWORKING|TOTAL_IND_SERVEI_DEGUSTACIO|TOTAL_IND_OBERT24H|TOTAL_IND_MIXT|TOTAL_IND_PEU_CARRER|TOTAL_IND_MERCAT|TOTAL_IND_GALERIA|TOTAL_IND_CENTRE_COMERCIAL|TOTAL_IND_EIX_COMERCIAL|\n",
      "+-----------------------+--------+-------------+-----+---------------------+-------------------+---------------------------+------------------+--------------+--------------------+----------------+-----------------+--------------------------+-----------------------+\n",
      "|                      0|       0|            0|    0|                    0|                  0|                          0|                 0|             0|                   0|               0|                0|                         0|                      0|\n",
      "+-----------------------+--------+-------------+-----+---------------------+-------------------+---------------------------+------------------+--------------+--------------------+----------------+-----------------+--------------------------+-----------------------+\n",
      "\n",
      "Unique activities: 2\n"
     ]
    }
   ],
   "source": [
    "df_comercial_dis = spark.read.parquet(f\"{exploitation}/comercial_indicators_district_2022.parquet\")\n",
    "\n",
    "# Print schema\n",
    "df_comercial_dis.printSchema()\n",
    "\n",
    "# Show sample rows\n",
    "df_comercial_dis.show(5)\n",
    "\n",
    "# Count total records\n",
    "print(\"Total rows:\", df_comercial_dis.count())\n",
    "\n",
    "# Check null values\n",
    "df_comercial_dis.select([count(when(col(c).isNull(), c)).alias(c) for c in df_comercial_dis.columns]).show()\n",
    "\n",
    "# Check number of unique activities\n",
    "print(\"Unique activities:\", df_comercial_dis.select(\"DES_ACTIVITAT_PRINCIPAL\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1061ec",
   "metadata": {},
   "source": [
    "### **Tourist Housing Licenses** (District Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b4a75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COD_DISTRICTE: string (nullable = true)\n",
      " |-- TOTAL: long (nullable = true)\n",
      "\n",
      "+-------------+-----+\n",
      "|COD_DISTRICTE|TOTAL|\n",
      "+-------------+-----+\n",
      "|            7|  218|\n",
      "|            3| 1134|\n",
      "|            8|   29|\n",
      "|            5|  491|\n",
      "|            6| 1055|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows: 10\n",
      "+-------------+-----+\n",
      "|COD_DISTRICTE|TOTAL|\n",
      "+-------------+-----+\n",
      "|            0|    0|\n",
      "+-------------+-----+\n",
      "\n",
      "Unique districts: 10\n"
     ]
    }
   ],
   "source": [
    "df_hut_dis = spark.read.parquet(f\"{exploitation}/hut_district_2022_1T.parquet\")\n",
    "\n",
    "# Print schema\n",
    "df_hut_dis.printSchema()\n",
    "\n",
    "# Show sample rows\n",
    "df_hut_dis.show(5)\n",
    "\n",
    "# Count total rows\n",
    "print(\"Total rows:\", df_hut_dis.count())\n",
    "\n",
    "# Check nulls\n",
    "df_hut_dis.select([count(when(col(c).isNull(), c)).alias(c) for c in df_hut_dis.columns]).show()\n",
    "\n",
    "# Check unique districts\n",
    "print(\"Unique districts:\", df_hut_dis.select(\"COD_DISTRICTE\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d801f05",
   "metadata": {},
   "source": [
    "Bassed on those SPARK query outputs we can see that the data has been transformed correctly and is ready for consumption."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab3_env)",
   "language": "python",
   "name": "lab3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
