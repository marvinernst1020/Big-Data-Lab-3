#!/usr/bin/env python

"""
Lab 3 - Data Exploitation Pipeline

Group: L3-T04 - Marvin Ernst, Oriol Gelabert, Alex Malo

This script implements the second step of the Data Management Backbone for Lab 3: 
it transforms cleaned data in the Formatted Zone into aggregated, spatially-aware outputs in the Exploitation Zone.

Steps:
1. Aggregation of cleaned data
2. Computation of KPIs
3. Save outputs to exploitation_zone/
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, countDistinct, when, sum as _sum, round
from pathlib import Path
import os
import re

# Initialize Spark:
spark = SparkSession.builder.appName("Lab3_Exploitation_Pipeline").getOrCreate()

# Define paths
project_root = Path(__file__).resolve().parent.parent
formatted = project_root / "formatted_zone"
exploitation = project_root / "exploitation_zone"
exploitation.mkdir(parents=True, exist_ok=True)

# Load and process household size data:
pattern = re.compile(r"household_(20\d{2})_\.parquet", re.IGNORECASE)
for file in os.listdir(formatted):
    match = pattern.match(file)
    if match:
        year = match.group(1)
        df = spark.read.parquet(str(formatted / file))
        df = df.withColumn("TOTAL_PERSONES_AGG", col("NUM_PERSONES_AGG") * col("NUM_VALOR"))

        # Aggregate by AEB:
        df.groupBy("COD_AEB", "NUM_PERSONES_AGG").agg(
            _sum("TOTAL_PERSONES_AGG").alias("TOTAL_PERSONES_AGG"),
            _sum("NUM_VALOR").alias("NUM_VALOR")
        ).write.parquet(f"{exploitation}/households_aeb_{year}.parquet", mode="overwrite")

        # Aggregate by neighborhood:
        df.groupBy("COD_BARRI", "DES_BARRI", "NUM_PERSONES_AGG").agg(
            _sum("TOTAL_PERSONES_AGG").alias("TOTAL_PERSONES_AGG"),
            _sum("NUM_VALOR").alias("NUM_VALOR")
        ).write.parquet(f"{exploitation}/households_barri_{year}.parquet", mode="overwrite")

        # Aggregate by district:
        df.groupBy("COD_DISTRICTE", "DES_DISTRICTE", "NUM_PERSONES_AGG").agg(
            _sum("TOTAL_PERSONES_AGG").alias("TOTAL_PERSONES_AGG"),
            _sum("NUM_VALOR").alias("NUM_VALOR")
        ).write.parquet(f"{exploitation}/households_districte_{year}.parquet", mode="overwrite")

# Load and process commercial indicators:
pattern = re.compile(r"comercial_(20\d{2})\.parquet", re.IGNORECASE)
indicator_cols = [
    "IND_OCI_NOCTURN", "IND_COWORKING", "IND_SERVEI_DEGUSTACIO", "IND_OBERT24H",
    "IND_MIXT", "IND_PEU_CARRER", "IND_MERCAT", "IND_GALERIA",
    "IND_CENTRE_COMERCIAL", "IND_EIX_COMERCIAL"
]

for file in os.listdir(formatted):
    match = pattern.match(file)
    if match:
        year = match.group(1)
        df = spark.read.parquet(str(formatted / file))

        for level, group_cols, name in [
            (["COD_DISTRICTE"], "district"),
            (["COD_DISTRICTE", "COD_BARRI"], "barri"),
            (["COD_DISTRICTE", "COD_BARRI", "COD_SECCIO_CENSAL"], "section")
        ]:
            agg = df.groupBy(*group_cols).agg(
                count("*").alias("TOTAL"),
                *[_sum(c).alias(f"TOTAL_{c}") for c in indicator_cols]
            )
            for c in indicator_cols:
                agg = agg.withColumn(f"PCT_{c}", round(100 * col(f"TOTAL_{c}") / col("TOTAL"), 3))
            agg.write.parquet(f"{exploitation}/comercial_indicators_{name}_{year}.parquet", mode="overwrite")

# Load and process tourist housing data:
pattern = re.compile(r"hut_(20\d{2})_(\dT)\.parquet", re.IGNORECASE)
for file in os.listdir(formatted):
    match = pattern.match(file)
    if match:
        year, term = match.group(1), match.group(2)
        df = spark.read.parquet(str(formatted / file))
        df.groupBy("COD_DISTRICTE").agg(count("*").alias("TOTAL")) \
            .write.parquet(f"{exploitation}/hut_district_{year}_{term}.parquet", mode="overwrite")
        df.groupBy("COD_DISTRICTE", "DES_BARRI").agg(count("*").alias("TOTAL")) \
            .write.parquet(f"{exploitation}/hut_barri_{year}_{term}.parquet", mode="overwrite")

print("Exploitation pipeline completed successfully.")
spark.stop()